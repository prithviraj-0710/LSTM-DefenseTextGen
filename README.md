# LSTM-DefenseTextGen
# **Next Word Predictor for DRDO Defense Science Journal Subset**

## **Project Overview**  
This project is a **Next Word Prediction Model** trained on a subset of the **DRDO Defense Science Journal** dataset, specifically from the paper:  
**"Predictive Factor Analysis of Air-to-Air Engagement Outcomes Using Air Combat Manoeuvring Instrumentation Data."**  
The goal of this model is to **predict the next word** in a given sequence using an **LSTM-based neural network**.

## **Dataset**  
The dataset used for this project is derived from the **DRDO Defense Science Journal**, specifically from the paper mentioned above. The text corpus was processed and tokenized to train the model effectively.

## **Features**  
- ğŸš€ Uses **Long Short-Term Memory (LSTM)** networks to capture the context of input sequences.  
- ğŸ“– **Trained on the DRDO Defense Science Journal subset**, ensuring domain-specific predictions.  
- ğŸ”  Implements **embedding layers** for word vector representation.  
- âš¡ Supports **real-time next-word prediction** for scientific and defense-related text.  
- ğŸ› ï¸ Utilizes **Keras and TensorFlow** for deep learning implementation.  

## **Usage**  

### **Training the Model**  
1. Load the dataset (**DRDO Defense Science Journal subset**).  
2. Preprocess the text (**tokenization, padding**).  
3. Train the **LSTM model** on the processed data.  
4. Save the trained model for future use.  

### **Running Predictions**  
To generate the next-word prediction:  
1. **Load the trained model.**  
2. **Input a partial sentence.**  
3. **The model predicts the most probable next word.**  

## **Model Architecture**  
- **ğŸ“Œ Embedding Layer**: Converts words into dense vectors of fixed size.  
- **ğŸ“Œ LSTM Layer**: Captures long-term dependencies in text.  
- **ğŸ“Œ Dropout Layer**: Prevents overfitting by randomly deactivating neurons.  
- **ğŸ“Œ Dense Output Layer**: Uses a softmax activation to predict the next word.  

## **Potential Enhancements**  
- ğŸ” **Fine-tuning** with a larger dataset.  
- ğŸ¤– Implementing **Transformer-based models** (e.g., GPT or BERT) for improved accuracy.  
- ğŸŒ Developing a **web-based interface** for real-time next-word prediction.  

---
